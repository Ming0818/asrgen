Speaker authentication systems are being deployed for security critical
applications in industries like banking, forensics, and home automation. Like
other domains, such industries have benefited from recent advancements in deep
learning that lead to improved accuracy and trainability of the speech
authentication systems.  Despite the improvement in the efficiency of these
systems, evidence shows that they can be susceptible to adversarial
attacks\cite{wu2015spoofing}, thus motivating a current focus on understanding
adversarial attacks (\cite{szegedy2013intriguing},
\cite{goodfellow2014explaining}), finding countermeasures to detect and deflect
them and designing systems that are provably correct with respect to
mathematically-specified requirements~\cite{seshia2016vai}.

Parallel to advancements in speech authentication, neural speech
\textit{generation} (the process of using deep neural networks to generate
speech) has also seen huge progress in recent years \cite{wang2017tacotron}.  The combination of these advancements begs a natural
question that has, to the best of our knowledge, not yet been answered:
\begin{center}
Are speech authentication systems robust \\to adversarial attacks by speech generative models?
\end{center}

Generative Adversarial Networks (GANs) are generative models that recently have
been used to produce incredibly authentic samples in a variety of fields. The
core idea of GANs, a minimax game played between a generator network and a
discriminator network, extends naturally to the field of speaker authentication
and spoofing. 

With regards to this question, we offer in this research the following contributions:
\begin{itemize}
\item We evaluate samples produced with SampleRNN and WaveNet in their ability to fool text-independent speaker recognizers.
\item We propose strategies for untargeted attacks using Generative Adversarial Networks.
\item We propose a semi-supervised approach for targeted attacks by modifying
    Wasserstein's GAN loss function.
\end{itemize}

% The main evaluation metric for deep models has always been their qualitative ability to produce human-sounding speech. In this research, we will argue that using speech authenticators as validation
