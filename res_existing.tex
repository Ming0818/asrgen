This section investigates the output of speaker recognition systems when
features computed from fake data generated with WaveNet and sampleRNN are used as
input. As described in \ref{sub:speaker_recognition}, we train speaker
recognition systems with several speakers, including the speakers used to train
WaveNet and sampleRNN. None of the WaveNet or sampleRNN samples produced with
our model or downloaded from the authors website were successful in targeted or 
untargetted attacks to our speaker recognition system.

\subsection{WaveNet} % (fold)
\label{sub:res-WaveNet}
We attempted to replicate the model described in~\cite{van2016wavenet} but,
unfortunately, we do not have access to Google's computing power nor to the
North American Speech dataset Google used to train the WaveNet model that
produced the samples referenced in \cite{van2016wavenet}. We therefore used
the data from CSTR VCTK to train speaker dependent WaveNet speech synthesis
models that converged to producing speech that resembled the speaker's voice but
sounded like babbling. 
% subsection WaveNet (end)
\subsection{sampleRNN} % (fold)
\label{sub:samplernn}
We also tested samples from sampleRNN \cite{mehri2016samplernn}, another
autoregressive architecture that has been successfully used to generate both
speech and music samples. SampleRNN uses a hierarchical structure of deep RNNs
to model dependencies in the sample sequence. Each deep RNN operates at a
different temporal resolution so as to model both long term and short term
dependencies. We generated samples from the three-tiered variant, trained on the
Blizzard 2013 dataset \cite{prahallad2013blizzard}, a 300 hour corpus of a
single female speaker's narration. We also downloaded 10 second samples from the
original paper's online repository at
\texttt{https://soundcloud.com/samplernn/sets}, which we qualitatively found to
have less noise than our generated ones. 
% subsection SampleRNN (end)
