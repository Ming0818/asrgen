In this section we will describe the datasets used, the data engineering
pipeline, including pre-processing and feature extraction, the speaker
recognition models evaluated, and the adversarial attacks investigated in this paper.

\subsection{Datasets}
In our experiments we use three speech datasets, as shown in  
in Table\ref{tbl:datasets}. The datasets used are public and provide audio clips
of different lengths, quality, language and content.

\begin{table}[!h]
\centering
\begin{tabular}{lllll}
                                                                     & \cellcolor[HTML]{C0C0C0}Speakers & \cellcolor[HTML]{C0C0C0}Language & \cellcolor[HTML]{C0C0C0}Duration & \cellcolor[HTML]{C0C0C0}Context \\ \cline{2-5} 
\multicolumn{1}{l|}{\cellcolor[HTML]{C0C0C0}2013 Blizzard} & 1                                & English                          & 73 h                             & Book narration                  \\
\multicolumn{1}{l|}{\cellcolor[HTML]{C0C0C0}CSTR VCTK}               & 109                              & English                          & 400 Sentences                    & Newspaper narration               \\
\multicolumn{1}{l|}{\cellcolor[HTML]{C0C0C0}2004 NIST}               & 100                              & Multiple                         & 5 min / speaker                  & Conversational phone speech.                          
\end{tabular}
\bigskip
\caption{Description of the datasets used in our experiments. }
\label{tbl:datasets}
\end{table}

\subsection{Pre-processing}
\label{sub:processdata}
Data pre-processing is dependent on the model being trained. For SampleRNN and
WaveNet, the raw audio is reduced to 16kHz and quantized using the $\mu$-law
companding transformation as referenced in~\cite{mehri2016samplernn}
and~\cite{van2016wavenet}. For the model based on the Wasserstein GAN,
we pre-process the data by converting it to 16kHz and removing silences by using
the WebRTC Voice Activity Detector (VAD) as referenced
in~\cite{zeidan2014webrtc}. For the CNN speaker recognition system, the data is
pre-processed by resampling to 16kHz when necessary and removing silences by
using the aforemetioned VAD. 

\subsection{Feature extraction}
SampleRNN and WaveNet operate at the sample level, i.e. waveform, thus requiring
no feature extraction. The features used for the neural speaker recognition
system are based on Mel-Spectrograms with dynamic range compression. The
Mel-Spectrogram is obtained by projecting a spectrogram onto a mel scale. We use
the python library librosa~\cite{mcfee2015librosa} to project the spectrogram
onto 64 mel bands, with window size equal to 1024 samples and hop size equal to
160 samples, i.e. 100ms long frames. Dynamic range compression is computed as
described in~\cite{lukic2016speaker}, with $log(1 + C*M)$, where $C$ is a
compression constant scalar set to $1000$ and $M$ is a matrix representing the
Mel-Spectrogram. Training the GAN is also done with Mel-Spectrograms of 64 bands each.
                        
